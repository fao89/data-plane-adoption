{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data Plane Adoption procedure","text":""},{"location":"#openstack-adoption","title":"OpenStack adoption","text":"<p>This is a procedure for adopting an OpenStack cloud.</p> <p>Perform the actions from the sub-documents in the following order:</p> <ul> <li> <p>Deploy podified backend services</p> </li> <li> <p>Copy MariaDB data</p> </li> <li> <p>Keystone adoption</p> </li> <li> <p>Glance adoption</p> </li> <li> <p>Adoption of other services</p> </li> </ul> <p>If you face issues during adoption, check the Troubleshooting document for common problems and solutions.</p>"},{"location":"#post-openstack-ceph-adoption","title":"Post-OpenStack Ceph adoption","text":"<p>If the environment includes Ceph and some of its services are collocated on the Controller hosts (\"internal Ceph\"), then Ceph services need to be moved out of Controller hosts as the last step of the OpenStack adoption. Follow this documentation:</p> <ul> <li>Ceph cluster migration (RBD)</li> </ul>"},{"location":"backend_services_deployment/","title":"Backend services deployment","text":"<p>The following instructions create OpenStackControlPlane CR with MariaDB and RabbitMQ deployed, and other services disabled. This will be the foundation of the podified control plane.</p> <p>In subsequent steps we'll import the original databases and then add podified OpenStack control plane services.</p>"},{"location":"backend_services_deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>The cloud which we want to adopt is up and running. It's on   OpenStack Wallaby release.</p> </li> <li> <p>The <code>openstack-operator</code> is deployed, but <code>OpenStackControlPlane</code> is   not deployed.</p> </li> </ul> <p>For developer/CI environments, the openstack operator can be deployed   by running <code>make openstack</code> inside   install_yamls   repo.</p> <p>For production environments, the deployment method will likely be   different.</p> <ul> <li>There are free PVs available to be claimed (for MariaDB and RabbitMQ).</li> </ul> <p>For developer/CI environments driven by install_yamls, make sure   you've run <code>make crc_storage</code>.</p>"},{"location":"backend_services_deployment/#variables","title":"Variables","text":"<ul> <li>Set the desired admin password for the podified deployment. This can   be the original deployment's admin password or something else.</li> </ul> <pre><code>ADMIN_PASSWORD=SomePassword\n</code></pre> <ul> <li>Set service password variables to match the original deployment.   Database passwords can differ in podified environment, but   synchronizing the service account passwords is a required step.</li> </ul> <p>E.g. in developer environments with TripleO Standalone, the   passwords can be extracted like this:</p> <pre><code>CINDER_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' CinderPassword:' | awk -F ': ' '{ print $2; }')\nGLANCE_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' GlancePassword:' | awk -F ': ' '{ print $2; }')\nIRONIC_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' IronicPassword:' | awk -F ': ' '{ print $2; }')\nNEUTRON_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' NeutronPassword:' | awk -F ': ' '{ print $2; }')\nNOVA_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' NovaPassword:' | awk -F ': ' '{ print $2; }')\nOCTAVIA_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' OctaviaPassword:' | awk -F ': ' '{ print $2; }')\nPLACEMENT_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' PlacementPassword:' | awk -F ': ' '{ print $2; }')\n</code></pre>"},{"location":"backend_services_deployment/#pre-checks","title":"Pre-checks","text":""},{"location":"backend_services_deployment/#procedure-backend-services-deployment","title":"Procedure - backend services deployment","text":"<ul> <li>Create OSP secret.</li> </ul> <p>The procedure for this will vary, but in developer/CI environments   we use install_yamls:</p> <pre><code># in install_yamls\nmake input\n</code></pre> <ul> <li>If the <code>$ADMIN_PASSWORD</code> is different than the already set password   in <code>osp-secret</code>, amend the <code>AdminPassword</code> key in the <code>osp-secret</code>   correspondingly:</li> </ul> <pre><code>oc set data secret/osp-secret \"AdminPassword=$ADMIN_PASSWORD\"\n</code></pre> <ul> <li>Set service account passwords in <code>osp-secret</code> to match the service   account passwords from original deployment:</li> </ul> <pre><code>oc set data secret/osp-secret \"CinderPassword=$CINDER_PASSWORD\"\noc set data secret/osp-secret \"GlancePassword=$GLANCE_PASSWORD\"\noc set data secret/osp-secret \"IronicPassword=$IRONIC_PASSWORD\"\noc set data secret/osp-secret \"NeutronPassword=$NEUTRON_PASSWORD\"\noc set data secret/osp-secret \"NovaPassword=$NOVA_PASSWORD\"\noc set data secret/osp-secret \"OctaviaPassword=$OCTAVIA_PASSWORD\"\noc set data secret/osp-secret \"PlacementPassword=$PLACEMENT_PASSWORD\"\n</code></pre> <ul> <li>Deploy OpenStackControlPlane. Make sure to only enable MariaDB and   RabbitMQ services. All other services must be disabled.</li> </ul> <pre><code>oc apply -f - &lt;&lt;EOF\napiVersion: core.openstack.org/v1beta1\nkind: OpenStackControlPlane\nmetadata:\n  name: openstack\nspec:\n  secret: osp-secret\n  storageClass: local-storage\n  mariadb:\n    template:\n      containerImage: quay.io/tripleozedcentos9/openstack-mariadb:current-tripleo\n      storageRequest: 500M\n  rabbitmq:\n    template:\n      replicas: 1\n\n  keystone:\n    enabled: false\n  cinder:\n    enabled: false\n  glance:\n    enabled: false\n  placement:\n    enabled: false\n  ovn:\n    enabled: false\n  ovs:\n    enabled: false\n  neutron:\n    enabled: false\n  nova:\n    enabled: false\nEOF\n</code></pre>"},{"location":"backend_services_deployment/#post-checks","title":"Post-checks","text":"<ul> <li>Check that MariaDB is running.</li> </ul> <pre><code>oc get pod mariadb-openstack -o jsonpath='{.status.phase}{\"\\n\"}'\n</code></pre>"},{"location":"ceph/","title":"Data Plane adoption - Ceph Migration","text":"<p>In this scenario, assuming Ceph is already &gt;= 5, either for HCI or dedicated Storage nodes, the daemons living in the OpenStack control plane should be moved/migrated into the existing external RHEL nodes (typically the compute nodes for an HCI environment or dedicated storage nodes in all the remaining use cases).</p>"},{"location":"ceph/#requirements","title":"Requirements","text":"<ul> <li>Ceph is &gt;= 5 and managed by cephadm/orchestrator</li> <li>Ceph NFS (ganesha) migrated from a TripleO based deployment to cephadm</li> <li>Both the Ceph public and cluster networks are propagated, via TripleO, to the target nodes</li> <li>Ceph Mons need to keep their IPs (to avoid cold migration).</li> </ul>"},{"location":"ceph/#scenario-1-migrate-mon-and-mgr-from-controller-nodes","title":"SCENARIO 1: Migrate mon and mgr from controller nodes","text":"<p>The goal of the first POC is to prove we are able to successfully drain a controller node, in terms of ceph daemons, and move them to a different node. The initial target of the POC is RBD only, which means we\u2019re going to move only mon and mgr daemons. For the purposes of this POC, we'll deploy a ceph cluster with only mon, mgrs, and osds to simulate the environment a customer will be in before starting the migration. The goal of the first POC is to ensure that: - We can keep the mon IP addresses moving them to the CephStorage nodes - We can drain the existing controller nodes and shutting them down - We can deploy additional monitors to the existing nodes, promoting them as   _admin nodes that can be used by administrators to manage the ceph cluster   and perform day2 operations against it - We can keep the cluster operational during the migration</p>"},{"location":"ceph/#prerequisites","title":"Prerequisites","text":"<p>The Storage Nodes should be configured to have both storage and storage_mgmt network to make sure we can use both Ceph public and cluster networks.</p> <p>This step is the only one where the interaction with TripleO is required. From 17+ we don\u2019t have to run any stack update, however, we have commands that should be performed to run os-net-config on the baremetal node and configure additional networks.</p> <p>Make sure the network is defined in metalsmith.yaml for the CephStorageNodes:</p> <pre><code>- name: CephStorage\n  count: 2\n  instances:\n    - hostname: oc0-ceph-0\n      name: oc0-ceph-0\n    - hostname: oc0-ceph-1\n      name: oc0-ceph-1\n  defaults:\n    networks:\n      - network: ctlplane\n        vif: true\n      - network: storage_cloud_0\n          subnet: storage_cloud_0_subnet\n      - network: storage_mgmt_cloud_0\n          subnet: storage_mgmt_cloud_0_subnet\n    network_config:\n      template: templates/single_nic_vlans/single_nic_vlans_storage.j2\n</code></pre> <p>Then run:</p> <pre><code>openstack overcloud node provision \\\n  -o overcloud-baremetal-deployed-0.yaml --stack overcloud-0 \\\n  --network-config -y --concurrency 2 /home/stack/metalsmith-0.yam\n</code></pre> <p>Verify that the storage network is running on the node:</p> <pre><code>(undercloud) [CentOS-9 - stack@undercloud ~]$ ssh heat-admin@192.168.24.14 ip -o -4 a\nWarning: Permanently added '192.168.24.14' (ED25519) to the list of known hosts.\n1: lo    inet 127.0.0.1/8 scope host lo\\       valid_lft forever preferred_lft forever\n5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\\       valid_lft forever preferred_lft forever\n6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\\       valid_lft forever preferred_lft forever\n7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\\       valid_lft forever preferred_lft forever\n8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\\       valid_lft forever preferred_lft forever\n</code></pre>"},{"location":"ceph/#migrate-mons-and-mgrs-on-the-two-existing-cephstorage-nodes","title":"Migrate mon(s) and mgr(s) on the two existing CephStorage nodes","text":"<p>Create a ceph spec based on the default roles with the mon/mgr on the controller nodes.</p> <pre><code>openstack overcloud ceph spec -o ceph_spec.yaml -y  \\\n   --stack overcloud-0     overcloud-baremetal-deployed-0.yaml\n</code></pre> <p>Deploy the Ceph cluster</p> <pre><code> openstack overcloud ceph deploy overcloud-baremetal-deployed-0.yaml \\\n    --stack overcloud-0 -o deployed_ceph.yaml \\\n    --network-data ~/oc0-network-data.yaml \\\n    --ceph-spec ~/ceph_spec.yaml\n</code></pre> <p>Note:</p> <p>The ceph_spec.yaml, which is the OSP generated description of the ceph cluster, will be used, later in the process, as the basic template required by cephadm to update the status/info of the daemons</p> <p>Check the status of the cluster</p> <pre><code>[ceph: root@oc0-controller-0 /]# ceph -s\n  cluster:\n    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum oc0-controller-0,oc0-controller-1,oc0-controller-2 (age 19m)\n    mgr: oc0-controller-0.xzgtvo(active, since 32m), standbys: oc0-controller-1.mtxohd, oc0-controller-2.ahrgsk\n    osd: 8 osds: 8 up (since 12m), 8 in (since 18m); 1 remapped pgs\n\n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   43 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n</code></pre> <pre><code>[ceph: root@oc0-controller-0 /]# ceph orch host ls\nHOST              ADDR           LABELS          STATUS\noc0-ceph-0        192.168.24.14  osd\noc0-ceph-1        192.168.24.7   osd\noc0-controller-0  192.168.24.15  _admin mgr mon\noc0-controller-1  192.168.24.23  _admin mgr mon\noc0-controller-2  192.168.24.13  _admin mgr mon\n</code></pre> <p>The goal of the next section is to migrate the oc0-controller-{1,2} daemons into oc0-ceph-{0,1} as the very basic scenario that demonstrates we can actually make this kind of migration using cephadm.</p>"},{"location":"ceph/#migrate-oc0-controller-1-into-oc0-ceph-0","title":"Migrate oc0-controller-1 into oc0-ceph-0","text":"<p>ssh into controller-0, then</p> <p><code>cephadm shell -v /home/ceph-admin/specs:/specs</code></p> <p>ssh into ceph-0, then</p> <p><code>sudo \u201cwatch podman ps\u201d  # watch the new mon/mgr being deployed here</code></p> <p>(optional) if mgr is active in the source node, then:</p> <pre><code>ceph mgr fail &lt;mgr instance&gt;\n</code></pre> <p>From the cephadm shell, remove the labels on oc0-controller-1</p> <pre><code>    for label in mon mgr _admin; do\n           ceph orch host rm label oc0-controller-1 $label;\n    done\n</code></pre> <p>Add the missing labels to oc0-ceph-0</p> <pre><code>[ceph: root@oc0-controller-0 /]#\n&gt; for label in mon mgr _admin; do ceph orch host label add oc0-ceph-0 $label; done\nAdded label mon to host oc0-ceph-0\nAdded label mgr to host oc0-ceph-0\nAdded label _admin to host oc0-ceph-0\n</code></pre> <p>Drain and force-remove the oc0-controller-1 node</p> <pre><code>[ceph: root@oc0-controller-0 /]# ceph orch host drain oc0-controller-1\nScheduled to remove the following daemons from host 'oc0-controller-1'\ntype                 id\n-------------------- ---------------\nmon                  oc0-controller-1\nmgr                  oc0-controller-1.mtxohd\ncrash                oc0-controller-1\n</code></pre> <pre><code>[ceph: root@oc0-controller-0 /]# ceph orch host rm oc0-controller-1 --force\nRemoved  host 'oc0-controller-1'\n\n[ceph: root@oc0-controller-0 /]# ceph orch host ls\nHOST              ADDR           LABELS          STATUS\noc0-ceph-0        192.168.24.14  osd\noc0-ceph-1        192.168.24.7   osd\noc0-controller-0  192.168.24.15  mgr mon _admin\noc0-controller-2  192.168.24.13  _admin mgr mon\n</code></pre> <p>If you have only 3 mon nodes, and the drain of the node doesn\u2019t work as expected (the containers are still there), then SSH to controller-1 and force-purge the containers in the node:</p> <pre><code>[root@oc0-controller-1 ~]# sudo podman ps\nCONTAINER ID  IMAGE                                                                                        COMMAND               CREATED         STATUS             PORTS       NAMES\n5c1ad36472bc  quay.io/ceph/daemon@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mon.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mon-oc0-controller-1\n3b14cc7bf4dd  quay.io/ceph/daemon@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mgr.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mgr-oc0-controller-1-mtxohd\n\n[root@oc0-controller-1 ~]# cephadm rm-cluster --fsid f6ec3ebe-26f7-56c8-985d-eb974e8e08e3 --force\n\n[root@oc0-controller-1 ~]# sudo podman ps\nCONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES\n</code></pre> <p>Note: cephadm rm-cluster on a node which is not part of the cluster anymore has the effect of removing all the containers and doing some cleanup on the filesystem.</p> <p>Before shutting the oc0-controller-1 down, move the ip address (on the same network) to the oc0-ceph-0 node:</p> <pre><code>mon_host = [v2:172.16.11.54:3300/0,v1:172.16.11.54:6789/0] [v2:172.16.11.121:3300/0,v1:172.16.11.121:6789/0] [v2:172.16.11.205:3300/0,v1:172.16.11.205:6789/0]\n\n[root@oc0-controller-1 ~]# ip -o -4 a\n1: lo    inet 127.0.0.1/8 scope host lo\\       valid_lft forever preferred_lft forever\n5: br-ex    inet 192.168.24.23/24 brd 192.168.24.255 scope global br-ex\\       valid_lft forever preferred_lft forever\n6: vlan100    inet 192.168.100.96/24 brd 192.168.100.255 scope global vlan100\\       valid_lft forever preferred_lft forever\n7: vlan12    inet 172.16.12.154/24 brd 172.16.12.255 scope global vlan12\\       valid_lft forever preferred_lft forever\n8: vlan11    inet 172.16.11.121/24 brd 172.16.11.255 scope global vlan11\\       valid_lft forever preferred_lft forever\n9: vlan13    inet 172.16.13.178/24 brd 172.16.13.255 scope global vlan13\\       valid_lft forever preferred_lft forever\n10: vlan70    inet 172.17.0.23/20 brd 172.17.15.255 scope global vlan70\\       valid_lft forever preferred_lft forever\n11: vlan1    inet 192.168.24.23/24 brd 192.168.24.255 scope global vlan1\\       valid_lft forever preferred_lft forever\n12: vlan14    inet 172.16.14.223/24 brd 172.16.14.255 scope global vlan14\\       valid_lft forever preferred_lft forever\n</code></pre> <p>On the oc0-ceph-0:</p> <pre><code>[heat-admin@oc0-ceph-0 ~]$ ip -o -4 a\n1: lo    inet 127.0.0.1/8 scope host lo\\       valid_lft forever preferred_lft forever\n5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\\       valid_lft forever preferred_lft forever\n6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\\       valid_lft forever preferred_lft forever\n7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\\       valid_lft forever preferred_lft forever\n8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\\       valid_lft forever preferred_lft forever\n[heat-admin@oc0-ceph-0 ~]$ sudo ip a add 172.16.11.121 dev vlan11\n[heat-admin@oc0-ceph-0 ~]$ ip -o -4 a\n1: lo    inet 127.0.0.1/8 scope host lo\\       valid_lft forever preferred_lft forever\n5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\\       valid_lft forever preferred_lft forever\n6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\\       valid_lft forever preferred_lft forever\n7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\\       valid_lft forever preferred_lft forever\n7: vlan11    inet 172.16.11.121/32 scope global vlan11\\       valid_lft forever preferred_lft forever\n8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\\       valid_lft forever preferred_lft forever\n</code></pre> <p>Poweroff oc0-controller-1.</p> <p>Add the new mon on oc0-ceph-0 using the old ip address:</p> <pre><code>[ceph: root@oc0-controller-0 /]# ceph orch daemon add mon oc0-ceph-0:172.16.11.121\nDeployed mon.oc0-ceph-0 on host 'oc0-ceph-0'\n</code></pre> <p>Check the new container in the oc0-ceph-0 node:</p> <pre><code>b581dc8bbb78  quay.io/ceph/daemon@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mon.oc0-ceph-0...  24 seconds ago  Up 24 seconds ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mon-oc0-ceph-0\n</code></pre> <p>On the cephadm shell, backup the existing ceph_spec.yaml, edit the spec removing any oc0-controller-1 entry, and replace it with oc0-ceph-0:</p> <pre><code>cp ceph_spec.yaml ceph_spec.yaml.bkp # backup the ceph_spec.yaml file\n\n[ceph: root@oc0-controller-0 specs]# diff -u ceph_spec.yaml.bkp ceph_spec.yaml\n\n--- ceph_spec.yaml.bkp  2022-07-29 15:41:34.516329643 +0000\n+++ ceph_spec.yaml      2022-07-29 15:28:26.455329643 +0000\n@@ -7,14 +7,6 @@\n - mgr\n service_type: host\n ---\n-addr: 192.168.24.12\n-hostname: oc0-controller-1\n-labels:\n-- _admin\n-- mon\n-- mgr\n-service_type: host\n----\n addr: 192.168.24.19\n hostname: oc0-controller-2\n labels:\n@@ -38,7 +30,7 @@\n placement:\n   hosts:\n   - oc0-controller-0\n-  - oc0-controller-1\n+  - oc0-ceph-0\n   - oc0-controller-2\n service_id: mon\n service_name: mon\n@@ -47,8 +39,8 @@\n placement:\n   hosts:\n   - oc0-controller-0\n-  - oc0-controller-1\n   - oc0-controller-2\n+  - oc0-ceph-0\n service_id: mgr\n service_name: mgr\n service_type: mgr\n</code></pre> <p>Apply the resulting spec:</p> <pre><code>ceph orch apply -i ceph_spec.yaml \n\n The result of 12 is having a new mgr deployed on the oc0-ceph-0 node, and the spec reconciled within cephadm\n\n[ceph: root@oc0-controller-0 specs]# ceph orch ls\nNAME                     PORTS  RUNNING  REFRESHED  AGE  PLACEMENT\ncrash                               4/4  5m ago     61m  *\nmgr                                 3/3  5m ago     69s  oc0-controller-0;oc0-ceph-0;oc0-controller-2\nmon                                 3/3  5m ago     70s  oc0-controller-0;oc0-ceph-0;oc0-controller-2\nosd.default_drive_group               8  2m ago     69s  oc0-ceph-0;oc0-ceph-1\n\n[ceph: root@oc0-controller-0 specs]# ceph -s\n  cluster:\n    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3\n    health: HEALTH_WARN\n            1 stray host(s) with 1 daemon(s) not managed by cephadm\n\n  services:\n    mon: 3 daemons, quorum oc0-controller-0,oc0-controller-2,oc0-ceph-0 (age 5m)\n    mgr: oc0-controller-0.xzgtvo(active, since 62m), standbys: oc0-controller-2.ahrgsk, oc0-ceph-0.hccsbb\n    osd: 8 osds: 8 up (since 42m), 8 in (since 49m); 1 remapped pgs\n\n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   43 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n</code></pre> <p>Fix the warning by refreshing the mgr:</p> <pre><code>ceph mgr fail oc0-controller-0.xzgtvo\n</code></pre> <p>And at this point the cluster is clean:</p> <pre><code>[ceph: root@oc0-controller-0 specs]# ceph -s\n  cluster:\n    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3\n    health: HEALTH_OK\n\n  services:\n    mon: 3 daemons, quorum oc0-controller-0,oc0-controller-2,oc0-ceph-0 (age 7m)\n    mgr: oc0-controller-2.ahrgsk(active, since 25s), standbys: oc0-controller-0.xzgtvo, oc0-ceph-0.hccsbb\n    osd: 8 osds: 8 up (since 44m), 8 in (since 50m); 1 remapped pgs\n\n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   43 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n</code></pre> <p>oc0-controller-1 has been removed and powered off without leaving traces on the ceph cluster.</p> <p>The same approach and the same steps can be applied to migrate oc0-controller-2 to oc0-ceph-1.</p>"},{"location":"ceph/#screen-recording","title":"Screen Recording:","text":"<ul> <li>Externalize a TripleO deployed Ceph cluster</li> </ul>"},{"location":"ceph/#whats-next","title":"What\u2019s next","text":""},{"location":"ceph/#useful-resources","title":"Useful resources","text":"<ul> <li>cephadm - deploy additional mon(s)</li> </ul>"},{"location":"glance_adoption/","title":"Glance adoption","text":"<p>Adopting Glance means that an existing <code>OpenStackControlPlane</code> CR, where Glance is supposed to be disabled, should be patched to start the service with the configuration parameters provided by the source environment.</p> <p>When the procedure is over, the expectation is to see the <code>GlanceAPI</code> service up and running: the <code>Keystone endpoints</code> should be updated and the same backend of the source Cloud will be available. If the conditions above are met, the adoption is considered concluded.</p> <p>This guide also assumes that:</p> <ol> <li>A <code>TripleO</code> environment (the source Cloud) is running on one side;</li> <li>A <code>SNO</code> / <code>CodeReadyContainers</code> is running on the other side;</li> <li>(optional) an internal/external <code>Ceph</code> cluster is reacheable by both <code>crc</code> and <code>TripleO</code></li> </ol>"},{"location":"glance_adoption/#prerequisites","title":"Prerequisites","text":"<ul> <li>Previous Adoption steps completed. Notably, MariaDB and Keystone   should be already adopted.</li> </ul>"},{"location":"glance_adoption/#procedure-glance-adoption","title":"Procedure - Glance adoption","text":"<p>As already done for Keystone, the Glance Adoption follows the same pattern.</p> <p>Patch OpenStackControlPlane to deploy Glance:</p> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  glance:\n    enabled: true\n    template:\n      databaseInstance: openstack\n      containerImage: quay.io/tripleozedcentos9/openstack-glance-api:current-tripleo\n      storageClass: \"local-storage\"\n      storageRequest: 10G\n      glanceAPIInternal:\n        containerImage: quay.io/tripleozedcentos9/openstack-glance-api:current-tripleo\n      glanceAPIExternal:\n        containerImage: quay.io/tripleozedcentos9/openstack-glance-api:current-tripleo\n'\n</code></pre> <p>However, if a Ceph backend is used, the <code>customServiceConfig</code> parameter should be used to inject the right configuration to the <code>GlanceAPI</code> instance.</p> <p>Make sure the Ceph related secret exists in the <code>openstack</code> namespace:</p> <pre><code>$ oc get secrets | grep ceph\nceph-conf-files\n</code></pre> <p>If it doesn't exist, create a <code>Secret</code> which contains the <code>Cephx</code> key and Ceph configuration file so that the Glance pod created by the operator can mount those files in <code>/etc/ceph</code>.</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ceph-client-conf\n  namespace: openstack\nstringData:\n  ceph.client.openstack.keyring: |\n    [client.openstack]\n        key = &lt;secret key&gt;\n        caps mgr = \"allow *\"\n        caps mon = \"profile rbd\"\n        caps osd = \"profile rbd pool=images\"\n  ceph.conf: |\n    [global]\n    fsid = 7a1719e8-9c59-49e2-ae2b-d7eb08c695d4\n    mon_host = 10.1.1.2,10.1.1.3,10.1.1.4\n</code></pre> <p>This secret will be used in the <code>extraVolumes</code> parameters to propagate the files to the <code>GlanceAPI</code> pods (both internal and external).</p> <p>Patch OpenStackControlPlane to deploy Glance:</p> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  glance:\n    enabled: true\n    template:\n      databaseInstance: openstack\n      containerImage: quay.io/tripleozedcentos9/openstack-glance-api:current-tripleo\n      customServiceConfig: |\n        [DEFAULT]\n        enabled_backends=default_backend:rbd\n        [glance_store]\n        default_backend=default_backend\n        [default_backend]\n        rbd_store_ceph_conf=/etc/ceph/ceph.conf\n        rbd_store_user=openstack\n        rbd_store_pool=images\n        store_description=Ceph glance store backend.\n      storageClass: \"local-storage\"\n      storageRequest: 10G\n      glanceAPIInternal:\n        containerImage: quay.io/tripleozedcentos9/openstack-glance-api:current-tripleo\n      glanceAPIExternal:\n        containerImage: quay.io/tripleozedcentos9/openstack-glance-api:current-tripleo\n  extraMounts:\n    - extraVol:\n      - propagation:\n        - Glance\n        extraVolType: Ceph\n        volumes:\n        - name: ceph\n          projected:\n            sources:\n            - secret:\n                name: ceph-conf-files\n        mounts:\n        - name: ceph\n          mountPath: \"/etc/ceph\"\n          readOnly: true\n'\n</code></pre>"},{"location":"glance_adoption/#post-checks","title":"Post-checks","text":""},{"location":"glance_adoption/#test-the-glance-service-from-the-openstack-cli","title":"Test the glance service from the OpenStack cli","text":"<p>Inspect the resulting glance pods:</p> <pre><code>sh-5.1# cat /etc/glance/glance.conf.d/01-custom.conf\n\n[DEFAULT]\nenabled_backends=default_backend:rbd,ceph1:rbd\nenabled_backends=default_backend:rbd\n[glance_store]\ndefault_backend=default_backend\n[default_backend]\nrbd_store_ceph_conf=/etc/ceph/ceph.conf\nrbd_store_user=openstack\nrbd_store_pool=images\nstore_description=Ceph glance store backend.\n\nsh-5.1# ls /etc/ceph/ceph*\n/etc/ceph/ceph.client.openstack.keyring  /etc/ceph/ceph.conf\n</code></pre> <p>Ceph secrets are properly mounted, at this point let's move to the openstack cli and check the service is active and the endpoints are properly updated.</p> <pre><code>(openstack)$ endpoint list | grep image\n\n| 569ed81064f84d4a91e0d2d807e4c1f1 | regionOne | glance       | image        | True    | internal  | http://glance-internal-openstack.apps-crc.testing   |\n| 5843fae70cba4e73b29d4aff3e8b616c | regionOne | glance       | image        | True    | public    | http://glance-public-openstack.apps-crc.testing     |\n| 709859219bc24ab9ac548eab74ad4dd5 | regionOne | glance       | image        | True    | admin     | http://glance-admin-openstack.apps-crc.testing      |\n</code></pre>"},{"location":"glance_adoption/#image-upload","title":"Image upload","text":"<p>We can test that an image can be created on from the adopted service.</p> <pre><code>(openstack)$ export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\n(openstack)$ export OS_CLOUD=adopted\n(openstack)$ curl -L -o /tmp/cirros-0.5.2-x86_64-disk.img http://download.cirros-cloud.net/0.5.2/cirros-0.5.2-x86_64-disk.img\n    qemu-img convert -O raw /tmp/cirros-0.5.2-x86_64-disk.img /tmp/cirros-0.5.2-x86_64-disk.img.raw\n    openstack image create --container-format bare --disk-format raw --file /tmp/cirros-0.5.2-x86_64-disk.img.raw cirros\n    openstack image list\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   273  100   273    0     0   1525      0 --:--:-- --:--:-- --:--:--  1533\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 15.5M  100 15.5M    0     0  17.4M      0 --:--:-- --:--:-- --:--:-- 17.4M\n\n+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n| Field            | Value                                                                                                                                      |\n+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n| container_format | bare                                                                                                                                       |\n| created_at       | 2023-01-31T21:12:56Z                                                                                                                       |\n| disk_format      | raw                                                                                                                                        |\n| file             | /v2/images/46a3eac1-7224-40bc-9083-f2f0cd122ba4/file                                                                                       |\n| id               | 46a3eac1-7224-40bc-9083-f2f0cd122ba4                                                                                                       |\n| min_disk         | 0                                                                                                                                          |\n| min_ram          | 0                                                                                                                                          |\n| name             | cirros                                                                                                                                     |\n| owner            | 9f7e8fdc50f34b658cfaee9c48e5e12d                                                                                                           |\n| properties       | os_hidden='False', owner_specified.openstack.md5='', owner_specified.openstack.object='images/cirros', owner_specified.openstack.sha256='' |\n| protected        | False                                                                                                                                      |\n| schema           | /v2/schemas/image                                                                                                                          |\n| status           | queued                                                                                                                                     |\n| tags             |                                                                                                                                            |\n| updated_at       | 2023-01-31T21:12:56Z                                                                                                                       |\n| visibility       | shared                                                                                                                                     |\n+------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n\n+--------------------------------------+--------+--------+\n| ID                                   | Name   | Status |\n+--------------------------------------+--------+--------+\n| 46a3eac1-7224-40bc-9083-f2f0cd122ba4 | cirros | active |\n+--------------------------------------+--------+--------+\n\n\n(openstack)$ oc rsh ceph\nsh-4.4$ ceph -s\nr  cluster:\n    id:     432d9a34-9cee-4109-b705-0c59e8973983\n    health: HEALTH_OK\n\n  services:\n    mon: 1 daemons, quorum a (age 4h)\n    mgr: a(active, since 4h)\n    osd: 1 osds: 1 up (since 4h), 1 in (since 4h)\n\n  data:\n    pools:   5 pools, 160 pgs\n    objects: 46 objects, 224 MiB\n    usage:   247 MiB used, 6.8 GiB / 7.0 GiB avail\n    pgs:     160 active+clean\n\nsh-4.4$ rbd -p images ls\n46a3eac1-7224-40bc-9083-f2f0cd122ba4\n</code></pre>"},{"location":"keystone_adoption/","title":"Keystone adoption","text":""},{"location":"keystone_adoption/#prerequisites","title":"Prerequisites","text":"<ul> <li>Previous Adoption steps completed. Notably, the service databases   must already be imported into the podified MariaDB.</li> </ul>"},{"location":"keystone_adoption/#variables","title":"Variables","text":"<p>(There are no shell variables necessary currently.)</p>"},{"location":"keystone_adoption/#pre-checks","title":"Pre-checks","text":""},{"location":"keystone_adoption/#procedure-keystone-adoption","title":"Procedure - Keystone adoption","text":"<ul> <li>Patch OpenStackControlPlane to deploy Keystone:</li> </ul> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  keystone:\n    enabled: true\n    template:\n      secret: osp-secret\n      containerImage: quay.io/tripleozedcentos9/openstack-keystone:current-tripleo\n      databaseInstance: openstack\n'\n</code></pre> <ul> <li>Create a clouds.yaml file to talk to adopted Keystone:</li> </ul> <pre><code>cat &gt; clouds-adopted.yaml &lt;&lt;EOF\nclouds:\n  adopted:\n    auth:\n      auth_url: http://keystone-public-openstack.apps-crc.testing\n      password: $ADMIN_PASSWORD\n      project_domain_name: Default\n      project_name: admin\n      user_domain_name: Default\n      username: admin\n    cacert: ''\n    identity_api_version: '3'\n    region_name: regionOne\n    volume_api_version: '3'\nEOF\n</code></pre> <ul> <li>Clean up old endpoints that still point to old control plane   (everything except Keystone endpoints):</li> </ul> <pre><code>export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\nexport OS_CLOUD=adopted\n\nopenstack endpoint list | grep ' cinderv3 ' | awk '{ print $2; }' | xargs openstack endpoint delete\nopenstack endpoint list | grep ' glance ' | awk '{ print $2; }' | xargs openstack endpoint delete\nopenstack endpoint list | grep ' neutron ' | awk '{ print $2; }' | xargs openstack endpoint delete\nopenstack endpoint list | grep ' nova ' | awk '{ print $2; }' | xargs openstack endpoint delete\nopenstack endpoint list | grep ' placement ' | awk '{ print $2; }' | xargs openstack endpoint delete\nopenstack endpoint list | grep ' swift ' | awk '{ print $2; }' | xargs openstack endpoint delete\n</code></pre>"},{"location":"keystone_adoption/#post-checks","title":"Post-checks","text":"<ul> <li>See that Keystone endpoints are defined and pointing to the podified   FQDNs:</li> </ul> <pre><code>export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\nexport OS_CLOUD=adopted\n\nopenstack endpoint list | grep keystone\n</code></pre>"},{"location":"mariadb_copy/","title":"MariaDB data copy","text":"<p>This document describes how to move the databases from the original OpenStack deployment to the MariaDB instances in the OpenShift cluster.</p>"},{"location":"mariadb_copy/#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Make sure the previous Adoption steps have been performed successfully.</p> </li> <li> <p>The OpenStackControlPlane resource must be already created at this point.</p> </li> <li> <p>Podified MariaDB and RabbitMQ are running. No other podified     control plane services are running.</p> </li> <li> <p>There must be network routability between:</p> <ul> <li> <p>The adoption host and the original MariaDB.</p> </li> <li> <p>The adoption host and the podified MariaDB.</p> </li> <li> <p>Note that this routability requirement may change in the   future, e.g. we may require routability from original MariaDB to   podified MariaDB.</p> </li> </ul> </li> </ul>"},{"location":"mariadb_copy/#variables","title":"Variables","text":"<p>Define the shell variables used in the steps below. The values are just illustrative, use values which are correct for your environment:</p> <pre><code>PODIFIED_MARIADB_IP=$(oc get -o yaml pod mariadb-openstack | grep podIP: | awk '{ print $2; }')\nMARIADB_IMAGE=quay.io/tripleozedcentos9/openstack-mariadb:current-tripleo\n\n# Use your environment's values for these:\nEXTERNAL_MARIADB_IP=192.168.24.3\nEXTERNAL_DB_ROOT_PASSWORD=$(cat ~/tripleo-standalone-passwords.yaml | grep ' MysqlRootPassword:' | awk -F ': ' '{ print $2; }')\nPODIFIED_DB_ROOT_PASSWORD=12345678\n</code></pre>"},{"location":"mariadb_copy/#pre-checks","title":"Pre-checks","text":"<ul> <li>Test connection to the original DB (show databases):</li> </ul> <pre><code>podman run -i --rm --userns=keep-id -u $UID -v $PWD:$PWD:z,rw -w $PWD $MARIADB_IMAGE \\\n    mysql -h \"$EXTERNAL_MARIADB_IP\" -uroot \"-p$EXTERNAL_DB_ROOT_PASSWORD\" -e 'SHOW databases;'\n</code></pre> <ul> <li>Run mysqlcheck on the original DB:</li> </ul> <pre><code>podman run -i --rm --userns=keep-id -u $UID -v $PWD:$PWD:z,rw -w $PWD $MARIADB_IMAGE \\\n    mysqlcheck --all-databases -h $EXTERNAL_MARIADB_IP -u root \"-p$EXTERNAL_DB_ROOT_PASSWORD\"\n</code></pre> <ul> <li>Test connection to podified DB (show databases):</li> </ul> <pre><code>oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \\\n    mysql -h \"$PODIFIED_MARIADB_IP\" -uroot \"-p$PODIFIED_DB_ROOT_PASSWORD\" -e 'SHOW databases;'\n</code></pre>"},{"location":"mariadb_copy/#procedure-stopping-control-plane-services","title":"Procedure - stopping control plane services","text":"<p>From each controller node it is necessary to stop the control-plane services to avoid inconsistencies in the data migrated for the data-plane adoption procedure.</p> <p>1- Connect to all the controller nodes and stop the control plane services.</p> <p>2- Stop the services.</p> <pre><code># Configure SSH variables to stop the services\n# in each controller node. For example:\n\nCONTROLLER1_SSH=\"ssh -F $ENV_DIR/director_standalone/vagrant_ssh_config vagrant@standalone\"\nCONTROLLER2_SSH=\":\"\nCONTROLLER3_SSH=\":\"\n\n# Update the services list to be stoped\n\nServicesToStop=(\"tripleo_horizon.service\"\n\"tripleo_keystone.service\"\n\"tripleo_cinder_api.service\"\n\"tripleo_glance_api.service\"\n\"tripleo_neutron_api.service\"\n\"tripleo_nova_api.service\"\n\"tripleo_placement_api.service\")\n\necho \"Stopping the OpenStack services\"\n\nfor service in ${ServicesToStop[*]}; do\necho \"Stopping the service: $service in each controller node\"\n$CONTROLLER1_SSH sudo systemctl stop $service\n$CONTROLLER2_SSH sudo systemctl stop $service\n$CONTROLLER3_SSH sudo systemctl stop $service\ndone\n</code></pre> <p>3- Make sure all the services are stopped</p>"},{"location":"mariadb_copy/#procedure-data-copy","title":"Procedure - data copy","text":"<ul> <li>Create a temporary folder to store DB dumps and make sure it's the   working directory for the following steps:</li> </ul> <pre><code>mkdir ~/adoption-db\ncd ~/adoption-db\n</code></pre> <ul> <li>Create a dump of the original databases:</li> </ul> <pre><code>podman run -i --rm --userns=keep-id -u $UID -v $PWD:$PWD:z,rw -w $PWD $MARIADB_IMAGE bash &lt;&lt;EOF\n\nmysql -h $EXTERNAL_MARIADB_IP -u root \"-p$EXTERNAL_DB_ROOT_PASSWORD\" -N -e 'show databases' | while read dbname; do\n    echo \"Dumping \\$dbname\"\n    mysqldump -h $EXTERNAL_MARIADB_IP -uroot \"-p$EXTERNAL_DB_ROOT_PASSWORD\" \\\n        --single-transaction --complete-insert --skip-lock-tables --lock-tables=0 \\\n        --databases \"\\$dbname\" \\\n        &gt; \"\\$dbname\".sql\ndone\n\nEOF\n</code></pre> <ul> <li>Restore the databases from .sql files into the podified MariaDB:</li> </ul> <pre><code>for dbname in cinder glance keystone nova_api nova_cell0 nova ovs_neutron placement; do\n    echo \"Restoring $dbname\"\n    oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \\\n       mysql -h \"$PODIFIED_MARIADB_IP\" -uroot \"-p$PODIFIED_DB_ROOT_PASSWORD\" &lt; \"$dbname.sql\"\ndone\n</code></pre>"},{"location":"mariadb_copy/#post-checks","title":"Post-checks","text":"<ul> <li>Check that the databases were imported correctly:</li> </ul> <pre><code>oc run mariadb-client --image $MARIADB_IMAGE -i --rm --restart=Never -- \\\n   mysql -h \"$PODIFIED_MARIADB_IP\" -uroot \"-p$PODIFIED_DB_ROOT_PASSWORD\" -e 'SHOW databases;'\n</code></pre>"},{"location":"other_services_adoption/","title":"Adoption of other services","text":"<p>This part of the guide adopts the remaining services that don't have a specific guide of their own. It is likely that as adoption gets developed further, services will be removed from here and put into their own guides (e.g. like Glance).</p>"},{"location":"other_services_adoption/#prerequisites","title":"Prerequisites","text":"<ul> <li>Previous Adoption steps completed.</li> </ul>"},{"location":"other_services_adoption/#variables","title":"Variables","text":"<p>(There are no shell variables necessary currently.)</p>"},{"location":"other_services_adoption/#pre-checks","title":"Pre-checks","text":""},{"location":"other_services_adoption/#procedure-adoption-of-other-services","title":"Procedure - Adoption of other services","text":"<ul> <li>Deploy the rest of control plane services:</li> </ul> <pre><code>oc patch openstackcontrolplane openstack --type=merge --patch '\nspec:\n  # cinder:\n  #   enabled: true\n  #   template:\n  #     cinderAPI:\n  #       replicas: 1\n  #       containerImage: quay.io/tripleozedcentos9/openstack-cinder-api:current-tripleo\n  #     cinderScheduler:\n  #       replicas: 1\n  #       containerImage: quay.io/tripleozedcentos9/openstack-cinder-scheduler:current-tripleo\n  #     cinderBackup:\n  #       replicas: 1\n  #       containerImage: quay.io/tripleozedcentos9/openstack-cinder-backup:current-tripleo\n  #     cinderVolumes:\n  #       volume1:\n  #         containerImage: quay.io/tripleozedcentos9/openstack-cinder-volume:current-tripleo\n  #         replicas: 1\n\n  placement:\n    enabled: true\n    template:\n      containerImage: quay.io/tripleozedcentos9/openstack-placement-api:current-tripleo\n      databaseInstance: openstack\n      secret: osp-secret\n\n  ovn:\n    enabled: true\n    template:\n      ovnDBCluster:\n        ovndbcluster-nb:\n          replicas: 1\n          containerImage: quay.io/tripleozedcentos9/openstack-ovn-nb-db-server:current-tripleo\n          dbType: NB\n          storageRequest: 10G\n        ovndbcluster-sb:\n          replicas: 1\n          containerImage: quay.io/tripleozedcentos9/openstack-ovn-sb-db-server:current-tripleo\n          dbType: SB\n          storageRequest: 10G\n      ovnNorthd:\n        replicas: 1\n        containerImage: quay.io/tripleozedcentos9/openstack-ovn-northd:current-tripleo\n\n  ovs:\n    enabled: true\n    template:\n      ovsContainerImage: \"quay.io/skaplons/ovs:latest\"\n      ovnContainerImage: \"quay.io/tripleozedcentos9/openstack-ovn-controller:current-tripleo\"\n      external-ids:\n        system-id: \"random\"\n        ovn-bridge: \"br-int\"\n        ovn-encap-type: \"geneve\"\n\n  neutron:\n    enabled: true\n    template:\n      databaseInstance: openstack\n      containerImage: quay.io/tripleozedcentos9/openstack-neutron-server:current-tripleo\n      secret: osp-secret\n\n  # nova:\n  #   enabled: true\n  #   template:\n  #     secret: osp-secret\n'\n</code></pre>"},{"location":"other_services_adoption/#post-checks","title":"Post-checks","text":"<ul> <li>See that service endpoints are defined:</li> </ul> <pre><code>export OS_CLIENT_CONFIG_FILE=clouds-adopted.yaml\nexport OS_CLOUD=adopted\n\nopenstack endpoint list\n</code></pre>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>This document contains information about various issues you might face and how to solve them.</p>"},{"location":"troubleshooting/#errimagepull-due-to-missing-authentication","title":"ErrImagePull due to missing authentication","text":"<p>The deployed containers pull the images from private containers registries that can potentially return authentication errors like: <code>Failed to pull image \"registry.redhat.io/rhosp-rhel9/openstack-rabbitmq:17.0\": rpc error: code = Unknown desc = unable to retrieve auth token: invalid username/password: unauthorized: Please login to the Red Hat Registry using your Customer Portal credentials.</code></p> <p>An example of a failed pod:</p> <pre><code>  Normal   Scheduled       3m40s                  default-scheduler  Successfully assigned openstack/rabbitmq-server-0 to worker0\n  Normal   AddedInterface  3m38s                  multus             Add eth0 [10.101.0.41/23] from ovn-kubernetes\n  Warning  Failed          2m16s (x6 over 3m38s)  kubelet            Error: ImagePullBackOff\n  Normal   Pulling         2m5s (x4 over 3m38s)   kubelet            Pulling image \"registry.redhat.io/rhosp-rhel9/openstack-rabbitmq:17.0\"\n  Warning  Failed          2m5s (x4 over 3m38s)   kubelet            Failed to pull image \"registry.redhat.io/rhosp-rhel9/openstack-rabbitmq:17.0\": rpc error: code  ... can be found here: https://access.redhat.com/RegistryAuthentication\n  Warning  Failed          2m5s (x4 over 3m38s)   kubelet            Error: ErrImagePull\n  Normal   BackOff         110s (x7 over 3m38s)   kubelet            Back-off pulling image \"registry.redhat.io/rhosp-rhel9/openstack-rabbitmq:17.0\"\n</code></pre> <p>In order to solve this issue we need to get a valid pull-secret from the official Red Hat console site, store this pull secret locally in a machine with access to the Kubernetes API (service node), and then run:</p> <pre><code>oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=&lt;pull_secret_location.json&gt;\n</code></pre> <p>The previous commmand will make available the authentication information in all the cluster's compute nodes, then trigger a new pod deployment to pull the container image with:</p> <pre><code>kubectl delete pod rabbitmq-server-0 -n openstack\n</code></pre> <p>And the pod should be able to pull the image successfully. For more inforation about what container registries requires what type of authentication, check the official docs.</p>"}]}